{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaab5b4",
   "metadata": {},
   "source": [
    "# 解耦 RAGFlow：核心 RAG 流程的简化 Notebook 实践\n",
    "\n",
    "RAGFlow 作为一个功能强大的工程项目，其代码为了确保鲁棒性而存在较高的耦合度，并包含了复杂的错误处理逻辑。这对于希望专注于理解核心 RAG 算法流程的学习者来说，无疑增加了难度和时间成本。\n",
    "\n",
    "为了帮助对 RAGFlow 底层 RAG 算法感兴趣的朋友们更清晰、高效地掌握其核心逻辑，本文提取并简化了 RAG 流程中的关键代码。我们将其整合到一个 Jupyter Notebook 中，你可以通过逐个运行代码单元，直观地体验和理解从文档处理到最终生成等各个环节的操作。\n",
    "\n",
    "**核心目标:** 通过实践这份简化代码，你可以在 Jupyter Notebook 中逐一运行 RAG 的关键组件，从而深入理解其核心步骤，为将来把相关工程代码整合到自己的项目中奠定坚实基础。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442c2f4",
   "metadata": {},
   "source": [
    "\n",
    "为了理解并简化 RAGFlow 的核心 RAG 流程，我们首先将其宏观上划分为**离线处理**（构建知识库）和**在线查询处理**（响应用户提问）两大阶段，并进一步细化为以下六个关键的技术步骤：\n",
    "\n",
    "1.  **文档解析 (Document Parsing):** 从原始文件（如 PDF）中提取结构化信息。\n",
    "2.  **文本分块 (Text Chunking):** 将长文本分割成大小适中的块。\n",
    "3.  **嵌入生成 (Embedding Generation):** 将文本块转换为向量。\n",
    "4.  **索引构建 (Indexing):** 将文本块及其向量存入检索引擎。\n",
    "5.  **检索 (Retrieval):** 根据用户问题查找相关的文本块。\n",
    "6.  **生成 (Generation):** 结合问题与上下文生成答案。\n",
    "\n",
    "![RAG组件示意图](./images/rag-flowchart.png)\n",
    "\n",
    "在梳理清楚这套标准流程之后，下一步便是深入 RAGFlow 的项目代码，定位这些步骤的具体实现。\n",
    "\n",
    "你可以先在 VScode 中将 RAGFlow 项目代码 clone 下来，使用命令 `git clone https://github.com/infiniflow/ragflow.git`；或者，你也可以直接访问其 GitHub 仓库页面 ([https://github.com/infiniflow/ragflow](https://github.com/infiniflow/ragflow))，然后点击键盘上的 '.' 键进入 GitHub 的在线开发者模式（web editor）。\n",
    "\n",
    "通过浏览整个项目的结构（如下图所示），你会发现与 RAG 功能最相关的主要是 `deepdoc`（负责文档解析）和 `rag`（负责 RAG 核心实现，包括文档解析、分块、嵌入、检索、生成逻辑）这两个模块。\n",
    "\n",
    "![RAGFlow 项目结构示意图](./images/ragflow-proj-structure.png)\n",
    "\n",
    "因此，我们接下来的代码拆解和封装工作将重点围绕这两个模块进行。\n",
    "\n",
    "rag/app/naive.py：主要功能是通过对deepdoc中各个文档格式的parser进行继承封装，比如对pdf parser基类的继承class Pdf(PdfParser)，其次就是chunk函数负责分类，其涉及到的依赖模块有`api`, `rag.nlp`, `rag.utils`模块，我们可以直接将对应的模块复制到我们的项目中（避免重新造轮子和保证能够顺利运行代码）。\n",
    "\n",
    "我们可以参考这个思路，继承已经写好的类的基础上再做些自定义的拓展。接下来就通过这个方式自己完成文档解析和文本分块。\n",
    "\n",
    "--- \n",
    "### 阶段一：文档解析 (PDF Focus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1db71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_component/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deepdoc.parser import PdfParser\n",
    "from timeit import default_timer as timer\n",
    "import logging \n",
    "from rag.nlp import rag_tokenizer, naive_merge, tokenize_table, tokenize_chunks, find_codec, concat_img, \\\n",
    "    naive_merge_docx, tokenize_chunks_docx\n",
    "\n",
    "import re\n",
    "\n",
    "class Pdf(PdfParser):\n",
    "    def __call__(self, filename, binary=None, from_page=0,\n",
    "                 to_page=100000, zoomin=3, callback=None):\n",
    "        start = timer()\n",
    "        first_start = start\n",
    "        callback(msg=\"OCR started\")\n",
    "        self.__images__(\n",
    "            filename if not binary else binary,\n",
    "            zoomin,\n",
    "            from_page,\n",
    "            to_page,\n",
    "            callback\n",
    "        )\n",
    "        callback(msg=\"OCR finished ({:.2f}s)\".format(timer() - start))\n",
    "        logging.info(\"OCR({}~{}): {:.2f}s\".format(from_page, to_page, timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._layouts_rec(zoomin)\n",
    "        callback(0.63, \"Layout analysis ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._table_transformer_job(zoomin)\n",
    "        callback(0.65, \"Table analysis ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._text_merge()\n",
    "        callback(0.67, \"Text merged ({:.2f}s)\".format(timer() - start))\n",
    "        tbls = self._extract_table_figure(True, zoomin, True, True)\n",
    "        # self._naive_vertical_merge()\n",
    "        self._concat_downward()\n",
    "        # self._filter_forpages()\n",
    "\n",
    "        logging.info(\"layouts cost: {}s\".format(timer() - first_start))\n",
    "        return [(b[\"text\"], self._line_tag(b, zoomin))\n",
    "                for b in self.boxes], tbls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6eb006",
   "metadata": {},
   "source": [
    "我们创建了一个自定义的 Pdf 类，它继承自 deepdoc 库中的 PdfParser，从而拥有其全部核心解析功能。\n",
    "\n",
    "我们通过实现 __call__ 方法，使得 Pdf 类的实例可以像函数一样被直接调用。当调用一个 Pdf 实例时，它会自动按顺序执行以下核心的 PDF 解析流程：\n",
    "\n",
    "1.  **OCR 处理:** 对 PDF 页面进行光学字符识别，提取图像中的文本。\n",
    "2.  **布局分析:** 识别文本块、标题、段落等布局结构。\n",
    "3.  **表格识别:** 检测并提取 PDF 中的表格数据。\n",
    "4.  **文本合并:** 将识别出的文本块按照逻辑顺序进行合并。\n",
    "5.  **表格/图片提取:** 单独提取表格和图片信息。\n",
    "\n",
    "这种设计将复杂的 PDF 解析步骤封装在 Pdf 类的调用操作中，简化了使用流程。同时也是将开源项目整合到自己项目中的方法之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796698b4",
   "metadata": {},
   "source": [
    "测试一下PDF文档解析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b7cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Miss outlines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理信息: OCR started\n",
      "处理进度: 60.0%\n",
      "处理信息: OCR finished (6.57s)\n",
      "处理进度: 63.0%\n",
      "处理信息: Layout analysis (3.66s)\n",
      "处理进度: 65.0%\n",
      "处理信息: Table analysis (0.16s)\n",
      "处理进度: 67.0%\n",
      "处理信息: Text merged (0.00s)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<table><caption>PERFORMANCEREQUIREMENTS</caption>\n",
       "<tr><td  >SECTION</td><td  >COMPLIANT (yes/no)</td></tr>\n",
       "<tr><td  >6.1 -Each helmet shall be accompanied by manufacturers'instructions explaining the proper method of size adjustment,use,care and useful service life guidelines.</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-Each helmet shall bear permanent markings in at least1.5mm(0.06in.)high letters stating thefollowing information:</td><td></td></tr>\n",
       "<tr><td  >6.2-name or identification mark of themanufacturer;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-the date ofmanufacture;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2- the American National Standard Designation;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2- the applicable Type and Class Designations;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-the approximate headsize range (see table 1).</td><td  >Yes</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "binary=None\n",
    "from_page=0\n",
    "to_page=100000\n",
    "# lang=\"Chinese\"\n",
    "lang=\"English\"\n",
    "is_english = lang.lower() == \"english\" \n",
    "\n",
    "pdf_parser = Pdf()\n",
    "\n",
    "filename = \"example-pdf/report.pdf\"\n",
    "# filename=\"example-pdf/首都在线个股研报.pdf\"\n",
    "\n",
    "def dummy(prog=None, msg=\"\"):\n",
    "    if prog:\n",
    "        print(f\"处理进度: {prog*100:.1f}%\")\n",
    "    if msg:\n",
    "        print(f\"处理信息: {msg}\")\n",
    "\n",
    "sections, tables = pdf_parser(filename if not binary else binary, from_page=from_page, to_page=to_page, callback=dummy)\n",
    "Markdown(tables[0][0][1])\n",
    "\n",
    "# tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6c644",
   "metadata": {},
   "source": [
    "---\n",
    "### 阶段二：文本分块 (Chunking)\n",
    "\n",
    "将从 PDF 中提取的长文本（`sections`）和表格（`tables`）分割成语义相关且大小合适的块 (Chunks)。这有助于后续的嵌入和检索。\n",
    "\n",
    "**核心逻辑:**\n",
    "\n",
    "1.  **表格处理:** 使用 `tokenize_table` 将提取的表格数据转换为适合 RAG 的格式。\n",
    "2.  **文本合并与分割:** 使用 `naive_merge` 方法，根据设定的 `chunk_token_num` (块的最大 Token 数) 和 `delimiter` (分隔符) 将连续的文本块 (`sections`) 合并再分割。\n",
    "3.  **块 Token 化:** 使用 `tokenize_chunks` 对生成的文本块进行 Token 化处理，并添加文档元数据。\n",
    "4.  **结果排序 (可选但推荐):** 根据文本块在原始文档中的位置 (页码、Y 坐标) 进行排序，保持上下文连贯性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa5daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(filename, binary=None, from_page=0, to_page=100000,\n",
    "          lang=\"Chinese\", callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Supported file formats are docx, pdf, excel, txt.\n",
    "        This method apply the naive ways to chunk files.\n",
    "        Successive text will be sliced into pieces using 'delimiter'.\n",
    "        Next, these successive pieces are merge into chunks whose token number is no more than 'Max token number'.\n",
    "    \"\"\"\n",
    "\n",
    "    is_english = lang.lower() == \"english\"  # is_english(cks)\n",
    "    parser_config = kwargs.get(\n",
    "        \"parser_config\", {\n",
    "            \"chunk_token_num\": 128, \"delimiter\": \"\\n!?。；！？\", \"layout_recognize\": \"DeepDOC\"})\n",
    "    doc = {\n",
    "        \"docnm_kwd\": filename,\n",
    "        \"title_tks\": rag_tokenizer.tokenize(re.sub(r\"\\.[a-zA-Z]+$\", \"\", filename))\n",
    "    }\n",
    "    doc[\"title_sm_tks\"] = rag_tokenizer.fine_grained_tokenize(doc[\"title_tks\"])\n",
    "    results = []\n",
    "    pdf_parser = None\n",
    "\n",
    "    pdf_parser = Pdf()\n",
    "    \n",
    "    sections, tables = pdf_parser(filename if not binary else binary, from_page=from_page, to_page=to_page,\n",
    "                                    callback=callback)\n",
    "\n",
    "    results = tokenize_table(tables, doc, is_english)\n",
    "    st = timer()\n",
    "    chunks = naive_merge(\n",
    "        sections, int(parser_config.get(\n",
    "            \"chunk_token_num\", 128)), parser_config.get(\n",
    "            \"delimiter\", \"\\n!?。；！？\"))\n",
    "    if kwargs.get(\"section_only\", False):\n",
    "        return chunks\n",
    "\n",
    "    results.extend(tokenize_chunks(chunks, doc, is_english, pdf_parser))\n",
    "    print(\"naive_merge({}): {}\".format(filename, timer() - st))\n",
    "    \n",
    "    # 按位置排序结果\n",
    "    for item in results:\n",
    "        if item['position_int']:\n",
    "            pos = list(item['position_int'][0]) # Convert tuple to list for modification\n",
    "            # 假设格式是 (page, x1, y1, x2, y2)\n",
    "            if len(pos) == 5 and pos[2] > pos[4]: # If y1 > y2, swap them\n",
    "                pos[2], pos[4] = pos[4], pos[2]\n",
    "            item['position_int'] = [tuple(pos)] + item['position_int'][1:] # Update with corrected tuple\n",
    "\n",
    "    # 使用 sorted 函数进行排序\n",
    "    # key 函数返回一个元组 (page_number, top_y_coordinate)\n",
    "    # sorted 会先按元组的第一个元素排序，如果相同，则按第二个元素排序\n",
    "    results = sorted(results, key=lambda item: (item['position_int'][0][0], item['position_int'][0][2]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bce1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Miss outlines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理信息: OCR started\n",
      "处理进度: 60.0%\n",
      "处理信息: OCR finished (6.64s)\n",
      "处理进度: 63.0%\n",
      "处理信息: Layout analysis (3.61s)\n",
      "处理进度: 65.0%\n",
      "处理信息: Table analysis (0.17s)\n",
      "处理进度: 67.0%\n",
      "处理信息: Text merged (0.00s)\n",
      "naive_merge(example-pdf/report.pdf): 0.05168387503363192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1464x1195>,\n",
       "  'page_num_int': [1, 1, 1, 1, 2],\n",
       "  'position_int': [(1, 294, 67, 55, 782),\n",
       "   (1, 216, 704, 139, 151),\n",
       "   (1, 174, 662, 167, 236),\n",
       "   (1, 182, 670, 322, 393),\n",
       "   (2, 65, 553, 86, 143)],\n",
       "  'top_int': [55, 139, 167, 322, 86],\n",
       "  'content_with_weight': 'INTERTEKTESTREPORTCORTLAND,NEWYORK130453933USROUTE11TESTREPORTNO.3096874-001ANSIZ89.1-2003 INITIALTESTINGOF SEONGANSAVECO.,LTD-SEONGANSAVE MODELNUMBER:FASHIONI TYPEIRENDERED TO:SEONGANSAVECO.LTD 318-2YANGJEONG-2-DONG,BUSANJIN-GU BUSAN,KOREAREP.OF(SOUTH)AbstractTheprotective capidentified as aSeongAnSave Co.,Ltd.,model Fashion I,submitted by the manufacturer,wasreceived inpristine conditionon08/11/06,08/30/06,and09/06/06,andwas evaluated in accordance with the requirements of ANSI Z89.1-2003 entitled,“American National Standard forIndustrial HeadProtection,”between 08/25/06 and 09/06/06.',\n",
       "  'content_ltks': 'intertektestreportcortland newyork130453933usroute11testreportno 3096874 001ansiz89 1 2003 initialtestingof seongansaveco ltd seongansav modelnumb fashioni typeirend to seongansaveco ltd 318 2yangjeong 2 dong busanjin gu busan korearep of south abstracttheprotect capidentifi a aseongansav co ltd model fashion i submit by the manufactur wasreceiv inpristin conditionon08 11 06 08 30 06 and09 06 06 andwa evalu in accord with the requir of ansi z89 1 2003 entitl american nation standard forindustri headprotect between 08 25 06 and 09 06 06',\n",
       "  'content_sm_ltks': 'intertektestreportcortland newyork130453933usroute11testreportno 3096874 001ansiz89 1 2003 initialtestingof seongansaveco ltd seongansav modelnumb fashioni typeirend to seongansaveco ltd 318 2yangjeong 2 dong busanjin gu busan korearep of south abstracttheprotect capidentifi a aseongansav co ltd model fashion i submit by the manufactur wasreceiv inpristin conditionon08 11 06 08 30 06 and09 06 06 andwa evalu in accord with the requir of ansi z89 1 2003 entitl american nation standard forindustri headprotect between 08 25 06 and 09 06 06'},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1467x1114>,\n",
       "  'page_num_int': [2, 2, 2],\n",
       "  'position_int': [(2, 65, 187, 157, 554),\n",
       "   (2, 65, 554, 227, 300),\n",
       "   (2, 66, 555, 355, 386)],\n",
       "  'top_int': [157, 227, 355],\n",
       "  'content_with_weight': 'Details of the instrument calibration are maintained in laboratory records. Themeasurement of uncertaintyis availableuponrequest.IntroductionThis report describes the results of the test program conducted in accordance with ANSI Z89.1-2003 entitled,“American National Standard for Industrial Head Protection,\" performed on specimens submitted by the manufacturer.Testing of the abovementioned protective caps began only upon Interteks’receipt of the signed quote. Intertek Testing Services,located in Cortland NY,conducted the test evaluations.ProductDescriptionIntertek received 30 production protective caps with date code(s): 6/06 and 7/06.The test sampleswereidentified asspecimens1-30.',\n",
       "  'content_ltks': 'detail of the instrument calibr are maintain in laboratori record themeasur of uncertaintyi availableuponrequest introductionthi report describ the result of the test program conduct in accord with ansi z89 1 2003 entitl american nation standard for industri head protect perform on specimen submit by the manufactur test of the abovement protect cap began onli upon intertek receipt of the sign quot intertek test servic locat in cortland ny conduct the test evalu productdescriptionintertek receiv 30 product protect cap with date code s 6 06 and 7 06 the test sampleswereidentifi asspecimens1 30',\n",
       "  'content_sm_ltks': 'detail of the instrument calibr are maintain in laboratori record themeasur of uncertaintyi availableuponrequest introductionthi report describ the result of the test program conduct in accord with ansi z89 1 2003 entitl american nation standard for industri head protect perform on specimen submit by the manufactur test of the abovement protect cap began onli upon intertek receipt of the sign quot intertek test servic locat in cortland ny conduct the test evalu productdescriptionintertek receiv 30 product protect cap with date code s 6 06 and 7 06 the test sampleswereidentifi asspecimens1 30'},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1467x1061>,\n",
       "  'page_num_int': [2, 3, 3, 4, 4, 4],\n",
       "  'position_int': [(2, 67, 456, 441, 556),\n",
       "   (3, 67, 556, 401, 429),\n",
       "   (3, 68, 557, 443, 458),\n",
       "   (4, 68, 557, 55, 67),\n",
       "   (4, 68, 557, 84, 96),\n",
       "   (4, 65, 554, 112, 139)],\n",
       "  'top_int': [441, 401, 443, 55, 84, 112],\n",
       "  'content_with_weight': 'AuthorizationThe testwas authorized by quote number 20060099, signed by KimSang Woo.INSTRUCTIONSANDMARKINGSFLAMMABILITYMethod:The protective capwas tested in accordance with Section 9.1 above the Static TestLine (STL)Requirements:Noflame shall bevisible5 seconds afterremoval of the testflame.Results:FORCETRANSMISSIONMethod:Theprotectivecapwas tested inaccordancewithSection9.2.Requirements:Theprotectivecap shall transmitan average force ofnotmore than3781N(850 Ibs.).No individual specimen shall transmit a force more than 4450N(1000 Ibs.).',\n",
       "  'content_ltks': 'authorizationth testwa author by quot number 20060099 sign by kimsang woo instructionsandmarkingsflammabilitymethod the protect capwa test in accord with section 91 abov the static testlin stl requir noflam shall bevisible5 second afterremov of the testflam result forcetransmissionmethod theprotectivecapwa test inaccordancewithsection9 2 requir theprotectivecap shall transmitan averag forc ofnotmor than3781n 850 ib no individu specimen shall transmit a forc more than 4450n 1000 ib',\n",
       "  'content_sm_ltks': 'authorizationth testwa author by quot number 20060099 sign by kimsang woo instructionsandmarkingsflammabilitymethod the protect capwa test in accord with section 91 abov the static testlin stl requir noflam shall bevisible5 second afterremov of the testflam result forcetransmissionmethod theprotectivecapwa test inaccordancewithsection9 2 requir theprotectivecap shall transmitan averag forc ofnotmor than3781n 850 ib no individu specimen shall transmit a forc more than 4450n 1000 ib'},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': \"<table><caption>PERFORMANCEREQUIREMENTS</caption>\\n<tr><td  >SECTION</td><td  >COMPLIANT (yes/no)</td></tr>\\n<tr><td  >6.1 -Each helmet shall be accompanied by manufacturers'instructions explaining the proper method of size adjustment,use,care and useful service life guidelines.</td><td  >Yes</td></tr>\\n<tr><td  >6.2-Each helmet shall bear permanent markings in at least1.5mm(0.06in.)high letters stating thefollowing information:</td><td></td></tr>\\n<tr><td  >6.2-name or identification mark of themanufacturer;</td><td  >Yes</td></tr>\\n<tr><td  >6.2-the date ofmanufacture;</td><td  >Yes</td></tr>\\n<tr><td  >6.2- the American National Standard Designation;</td><td  >Yes</td></tr>\\n<tr><td  >6.2- the applicable Type and Class Designations;</td><td  >Yes</td></tr>\\n<tr><td  >6.2-the approximate headsize range (see table 1).</td><td  >Yes</td></tr>\\n</table>\",\n",
       "  'content_ltks': 'performancerequir section compliant ye no 61 each helmet shall be accompani by manufactur instruct explain the proper method of size adjust use care and use servic life guidelin ye 6 2 each helmet shall bear perman mark in at least1 5mm 0 06in high letter state thefollow inform 6 2 name or identif mark of themanufactur ye 6 2 the date ofmanufactur ye 6 2 the american nation standard design ye 6 2 the applic type and class design ye 6 2 the approxim headsiz rang see tabl 1 ye',\n",
       "  'content_sm_ltks': 'performancerequir section compliant ye no 61 each helmet shall be accompani by manufactur instruct explain the proper method of size adjust use care and use servic life guidelin ye 6 2 each helmet shall bear perman mark in at least1 5mm 0 06in high letter state thefollow inform 6 2 name or identif mark of themanufactur ye 6 2 the date ofmanufactur ye 6 2 the american nation standard design ye 6 2 the applic type and class design ye 6 2 the approxim headsiz rang see tabl 1 ye',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1441x621>,\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 71, 302, 95, 551)],\n",
       "  'top_int': [95]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': '<table>\\n<tr><td  >SPECIMEN</td><td  >LOCATION</td><td  >AFTERFLAME(sec.)</td><td  >COMPLIANT</td></tr>\\n<tr><td  >12</td><td  >Rear</td><td  >0.0</td><td  >Yes</td></tr>\\n</table>',\n",
       "  'content_ltks': 'specimen locat afterflam sec compliant 12 rear 0 0 ye',\n",
       "  'content_sm_ltks': 'specimen locat afterflam sec compliant 12 rear 0 0 ye',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1479x112>,\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 62, 533, 496, 555)],\n",
       "  'top_int': [496]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1462x1065>,\n",
       "  'page_num_int': [4, 5, 5, 5, 5, 5, 5],\n",
       "  'position_int': [(4, 65, 196, 182, 553),\n",
       "   (5, 65, 553, 83, 98),\n",
       "   (5, 65, 553, 111, 139),\n",
       "   (5, 390, 878, 183, 195),\n",
       "   (5, 65, 553, 184, 196),\n",
       "   (5, 64, 552, 338, 351),\n",
       "   (5, 63, 551, 366, 380)],\n",
       "  'top_int': [182, 83, 111, 183, 184, 338, 366],\n",
       "  'content_with_weight': 'Results:VEL0CITYRANGE:5.45-5.55m/s(5.50±0.05m/s)DROPHEIGHT:60.7inchesAPEXPENETRATIONMethod:The protective cap was tested in accordancewith Section9.3.Requirements:The penetrator shall not make contactwith the top of the test headform under any ofthetest conditions specified.Results:DROPHEIGHT:98.3inchesVELOCITYRANGE:6.9-7.1m/s(7.0±0.1m/s)ELECTRICALINSULATION (ClaSSG)Method:The protective capwas testedin accordancewithSection9.7.',\n",
       "  'content_ltks': 'result vel0cityrang 5 45 5 55m s 5 500 05m s dropheight 60 7inchesapexpenetrationmethod the protect cap wa test in accordancewith section9 3 requir the penetr shall not make contactwith the top of the test headform under ani ofthetest condit specifi result dropheight 98 3inchesvelocityrang 69 7 1m s 700 1m s electricalinsul classg method the protect capwa testedin accordancewithsection9 7',\n",
       "  'content_sm_ltks': 'result vel0cityrang 5 45 5 55m s 5 500 05m s dropheight 60 7inchesapexpenetrationmethod the protect cap wa test in accordancewith section9 3 requir the penetr shall not make contactwith the top of the test headform under ani ofthetest condit specifi result dropheight 98 3inchesvelocityrang 69 7 1m s 700 1m s electricalinsul classg method the protect capwa testedin accordancewithsection9 7'},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': '<table><caption>RESULTS:COMPLIANT</caption>\\n<tr><th colspan=3 >120±3.6F SPECIMEN VELOCITY FORCE NO. (m/s) (lbs.)</th><th colspan=3 >0±3.6°F SPECIMEN VELOCITY FORCE NO. (m/s) (Ibs.)</th></tr>\\n<tr><td  >1</td><td  >5.59</td><td  >621.93</td><td  >13</td><td  >5.50</td><td  >763.08</td></tr>\\n<tr><td  >2</td><td  >5.50</td><td  >621.93</td><td  >14</td><td  >5.50</td><td  >670.45</td></tr>\\n<tr><td  >3</td><td  >5.50</td><td  >617.52</td><td  >15</td><td  >5.50</td><td  >939.51</td></tr>\\n<tr><td  >4</td><td  >5.50</td><td  >608.70</td><td  >16</td><td  >5.50</td><td  >899.91</td></tr>\\n<tr><td  >5</td><td  >5.50</td><td  >635.16</td><td  >17</td><td  >5.51</td><td  >833.65</td></tr>\\n<tr><td  >6</td><td  >5.50</td><td  >630.75</td><td  >18</td><td  >5.51</td><td  >696.91</td></tr>\\n<tr><td  >7</td><td  >5.50</td><td  >599.88</td><td  >19</td><td  >5.51</td><td  >913.05</td></tr>\\n<tr><td  >8</td><td  >5.50</td><td  >635.16</td><td  >20</td><td  >5.51</td><td  >820.42</td></tr>\\n<tr><td  >9</td><td  >5.50</td><td  >617.52</td><td  >21</td><td  >5.51</td><td  >926.28</td></tr>\\n<tr><td  >10</td><td  >5.50</td><td  >626.34</td><td  >22</td><td  >5.51</td><td  >771.90</td></tr>\\n<tr><td  >11</td><td  >5.59</td><td  >626.34</td><td  >23</td><td  >5.51</td><td  >939.51</td></tr>\\n<tr><td  >12</td><td  >5.50</td><td  >626.34</td><td  >24</td><td  >5.51</td><td  >771.90</td></tr>\\n<tr><td></td><td  >AVERAGE</td><td  >622.30</td><td></td><td  >AVERAGE</td><td  >828.88</td></tr>\\n</table>',\n",
       "  'content_ltks': 'result compliant 120 3 6f specimen veloc forc no m s lb 0 36 f specimen veloc forc no m s ib 15 59 621 93 135 50 763 08 25 50 621 93 14 5 50 670 45 35 50 617 52 15 5 50 939 51 45 50 608 70 16 5 50 899 91 55 50 635 16 17 5 51 833 65 65 50 630 75 18 5 51 696 91 75 50 599 88 19 5 51 913 05 85 50 635 16 20 5 51 820 42 95 50 617 52 21 5 51 926 28 10 5 50 626 34 22 5 51 771 90 115 59 626 34 23 5 51 939 51 12 5 50 626 34 24 5 51 771 90 averag 622 30 averag 828 88',\n",
       "  'content_sm_ltks': 'result compliant 120 3 6f specimen veloc forc no m s lb 0 36 f specimen veloc forc no m s ib 15 59 621 93 135 50 763 08 25 50 621 93 14 5 50 670 45 35 50 617 52 15 5 50 939 51 45 50 608 70 16 5 50 899 91 55 50 635 16 17 5 51 833 65 65 50 630 75 18 5 51 696 91 75 50 599 88 19 5 51 913 05 85 50 635 16 20 5 51 820 42 95 50 617 52 21 5 51 926 28 10 5 50 626 34 22 5 51 771 90 115 59 626 34 23 5 51 939 51 12 5 50 626 34 24 5 51 771 90 averag 622 30 averag 828 88',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1468x810>,\n",
       "  'page_num_int': [4],\n",
       "  'position_int': [(4, 60, 479, 210, 549)],\n",
       "  'top_int': [210]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': '<table>\\n<tr><th></th><th  >HOTCONDITION</th><th></th><th></th><th  >COLDCONDITION</th><th></th></tr>\\n<tr><th  >SPECIMEN</th><th  >VELOCITY</th><th  >COMPLIANT</th><th  >SPECIMEN</th><th  >VELOCITY</th><th  >COMPLIANT</th></tr>\\n<tr><th  >NO.</th><th  >(m/s)</th><th  >(yes/no)</th><th  >NO.</th><th  >(m/s)</th><th  >(yes/no)</th></tr>\\n<tr><td  >25</td><td  >7.08</td><td  >Yes</td><td  >28</td><td  >6.94</td><td  >Yes</td></tr>\\n<tr><td  >26</td><td  >6.94</td><td  >Yes</td><td  >29</td><td  >6.94</td><td  >Yes</td></tr>\\n<tr><td  >27</td><td  >6.94</td><td  >Yes</td><td  >30</td><td  >7.08</td><td  >Yes</td></tr>\\n</table>',\n",
       "  'content_ltks': 'hotcondit coldcondit specimen veloc compliant specimen veloc compliant no m s ye no no m s ye no 25 7 08 ye 286 94 ye 26 6 94 ye 29 6 94 ye 27 6 94 ye 30 7 08 ye',\n",
       "  'content_sm_ltks': 'hotcondit coldcondit specimen veloc compliant specimen veloc compliant no m s ye no no m s ye no 25 7 08 ye 286 94 ye 26 6 94 ye 29 6 94 ye 27 6 94 ye 30 7 08 ye',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1516x303>,\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 60, 310, 209, 565)],\n",
       "  'top_int': [209]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1464x1189>,\n",
       "  'page_num_int': [5, 5, 5, 5, 6],\n",
       "  'position_int': [(5, 63, 422, 393, 551),\n",
       "   (5, 58, 546, 543, 556),\n",
       "   (5, 55, 543, 570, 585),\n",
       "   (5, 55, 543, 599, 641),\n",
       "   (6, 65, 553, 81, 137)],\n",
       "  'top_int': [393, 543, 570, 599, 81],\n",
       "  'content_with_weight': 'Requirements:Theprotective cap shall withstand2,200volts（rootmean square),AC,60Hertz,for1minutes.Leakage shall not exceed 3milliamperes.Results:ELECTRICALINSULATION (ClaSSE)Method:Theprotective capwas tested in accordance with Section9.7.Requirements:Theprotectivecap shall withstand 20,000volts（rootmean square),AC,60 Hertz,for3minutes.Leakage shall not exceed9milliamperes.At 30,000volts,the test sample shallnotburn through.Results:ConclusionThe protective cap identified as a Seong An Save,model Fashion I,met the minimum performancerequirements definedin ANSI Z89.1-2003 entitled,“American National Standard for Industrial HeadProtection”.The helmet type and class defined for this modelas a result of the evaluationsperformed in this report are determined tobeTypeI-Class G,E,& C.',\n",
       "  'content_ltks': 'requir theprotect cap shall withstand2 200volt rootmean squar ac 60hertz for1minut leakag shall not exceed 3milliamper result electricalinsul class method theprotect capwa test in accord with section9 7 requir theprotectivecap shall withstand 20 000volt rootmean squar ac 60 hertz for3minut leakag shall not exceed9milliamper at 30 000volt the test sampl shallnotburn through result conclusionth protect cap identifi a a seong an save model fashion i met the minimum performancerequir definedin ansi z89 1 2003 entitl american nation standard for industri headprotect the helmet type and class defin for thi modela a result of the evaluationsperform in thi report are determin tobetypei class g e c',\n",
       "  'content_sm_ltks': 'requir theprotect cap shall withstand2 200volt rootmean squar ac 60hertz for1minut leakag shall not exceed 3milliamper result electricalinsul class method theprotect capwa test in accord with section9 7 requir theprotectivecap shall withstand 20 000volt rootmean squar ac 60 hertz for3minut leakag shall not exceed9milliamper at 30 000volt the test sampl shallnotburn through result conclusionth protect cap identifi a a seong an save model fashion i met the minimum performancerequir definedin ansi z89 1 2003 entitl american nation standard for industri headprotect the helmet type and class defin for thi modela a result of the evaluationsperform in thi report are determin tobetypei class g e c'},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': '<table>\\n<tr><th  >SPECIMENNO.</th><th  >LEAKAGE(mA)</th><th  >COMPLIANT</th></tr>\\n<tr><td  >1</td><td  >0.51</td><td  >Yes</td></tr>\\n<tr><td  >13</td><td  >0.42</td><td  >Yes</td></tr>\\n</table>',\n",
       "  'content_ltks': 'specimenno leakag ma compliant 10 51 ye 13 0 42 ye',\n",
       "  'content_sm_ltks': 'specimenno leakag ma compliant 10 51 ye 13 0 42 ye',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1494x164>,\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 57, 517, 462, 555)],\n",
       "  'top_int': [462]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'content_with_weight': '<table>\\n<tr><th  >SPECIMENNO.</th><th  >LEAKAGE(mA)</th><th  >COMPLIANT</th></tr>\\n<tr><td  >1</td><td  >5.26</td><td  >Yes</td></tr>\\n<tr><td  >13</td><td  >4.18</td><td  >Yes</td></tr>\\n</table>',\n",
       "  'content_ltks': 'specimenno leakag ma compliant 15 26 ye 13 4 18 ye',\n",
       "  'content_sm_ltks': 'specimenno leakag ma compliant 15 26 ye 13 4 18 ye',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1493x164>,\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 51, 548, 681, 736)],\n",
       "  'top_int': [681]},\n",
       " {'docnm_kwd': 'example-pdf/report.pdf',\n",
       "  'title_tks': 'exampl pdf report',\n",
       "  'title_sm_tks': 'exampl pdf report',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=314x834>,\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 62, 166, 251, 295)],\n",
       "  'top_int': [251],\n",
       "  'content_with_weight': 'ReportReviewedby:TestPerformedby:DeanMoran Technician PerformanceGroupSaraOseid Technician PerformanceGroup',\n",
       "  'content_ltks': 'reportreviewedbi testperformedbi deanmoran technician performancegroupsaraoseid technician performancegroup',\n",
       "  'content_sm_ltks': 'reportreviewedbi testperformedbi deanmoran technician performancegroupsaraoseid technician performancegroup'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents = chunk(filename, binary=None, from_page=0, to_page=100000, callback=dummy, parser_config={\"chunk_token_num\": 128, \"delimiter\": \"\\n!?。；！？\", \"layout_recognize\": \"DeepDOC\"})\n",
    "chunked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1aa7ad",
   "metadata": {},
   "source": [
    "--- \n",
    "### 阶段三：嵌入生成 (Embedding Generation)\n",
    "\n",
    "将每个文本块转换为一个数值向量（Embedding），这个向量代表了文本块的语义信息。相似语义的文本块会有相似的向量表示。\n",
    "\n",
    "**核心逻辑:**\n",
    "\n",
    "1.  **选择嵌入模型:** 确定使用的嵌入模型。\n",
    "2.  **配置 API 客户端:** 设置访问模型服务的 API Key 和 Base URL。\n",
    "3.  **调用嵌入接口:** 对每个文本块的内容 (`content_with_weight`) 调用嵌入模型的 API，获取其向量表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98e4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# === 加载 .env 文件 ===\n",
    "load_dotenv()\n",
    "\n",
    "# === 嵌入生成函数 ===\n",
    "def generate_embedding(\n",
    "    text: str,\n",
    "    api_key: str = os.getenv(\"EMBEDDING_API_KEY\", \"your_api_key_here\"),\n",
    "    base_url: str = os.getenv(\"EMBEDDING_BASE_URL\", \"your_embedding_api_url_here\"),\n",
    "    model_name: str = os.getenv(\"EMBEDDING_MODEL_NAME\", \"your_embedding_model_name\"),\n",
    "    dimensions: int = 1024, \n",
    "    encoding_format: str = \"float\"\n",
    ") -> list[float] | None:\n",
    "    \"\"\"\n",
    "    使用 OpenAI 兼容的 API 为给定文本生成嵌入。\n",
    "\n",
    "    Args:\n",
    "        text: 需要嵌入的输入文本。\n",
    "        api_key: 嵌入服务的 API 密钥。\n",
    "        base_url: 嵌入服务的基础 URL。\n",
    "        model_name: 要使用的嵌入模型的名称。\n",
    "\n",
    "    Returns:\n",
    "        代表嵌入向量的浮点数列表，如果发生错误则返回 None。\n",
    "    \"\"\"\n",
    "    # 验证输入文本是否为非空字符串\n",
    "    if not text or not isinstance(text, str):\n",
    "        logging.warning(\"无效的文本输入，无法生成嵌入。返回 None。\")\n",
    "        return None\n",
    "        \n",
    "    # 初始化 OpenAI 客户端\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    "    )\n",
    "    # 调用 OpenAI 的嵌入接口\n",
    "    try:\n",
    "        completion = client.embeddings.create(\n",
    "            model=model_name,\n",
    "            input=text,\n",
    "            dimensions=dimensions,\n",
    "            encoding_format=encoding_format\n",
    "        )\n",
    "\n",
    "        embedding = completion.data[0].embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API 请求失败: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8f7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_embedding(text: str, api_key: str = None, base_url: str = None, model_name: str = \"text-embedding-v3\", dimensions: int = 1024, encoding_format: str = \"float\"):\n",
    "    api_key = \"lm-studio\"\n",
    "    base_url = \"http://127.0.0.1:1234/v1\"    \n",
    "    model_name = \"text-embedding-bge-m3\"\n",
    "\n",
    "    # 初始化 OpenAI 客户端\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    "    )\n",
    "\n",
    "    # 调用 OpenAI 的嵌入接口\n",
    "    try:\n",
    "        completion = client.embeddings.create(\n",
    "            model=model_name,\n",
    "            input=text,\n",
    "            dimensions=dimensions,\n",
    "            encoding_format=encoding_format\n",
    "        )\n",
    "\n",
    "        embedding = completion.data[0].embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API 请求失败: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e1b79",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "### 阶段四：索引构建 (Indexing with Elasticsearch)\n",
    "\n",
    "RAGFlow采用的是Elasticsearch作为向量数据库，主要因为它是开源数据库当中，为数不多可以兼具全文搜索和向量搜索两路混合召回能力的数据库。\n",
    "\n",
    "**相关资源**:\n",
    "\n",
    "* Elasticsearch 安装 (macOS 参考):\n",
    "\n",
    "    - https://mp.weixin.qq.com/s/XN-XrRt5S_JD5KNLRQrXzg\n",
    "\n",
    "    - https://mp.weixin.qq.com/s/1hBUOvmW7sbPqmlTAD_R9w\n",
    "\n",
    "* Elasticsearch 搜索方法参考:\n",
    "\n",
    "    - https://github.com/elastic/elasticsearch-labs/tree/main/notebooks\n",
    "\n",
    "本阶段需掌握 ES 的索引创建、批量插入、关键词及向量检索等基本操作。\n",
    "\n",
    "**目标:** 将处理好的文本块（含元数据和向量）存入 Elasticsearch，以便快速检索。\n",
    "\n",
    "execute_insert_process 函数负责编排整个流程：文档解析 -> 文本分块 -> 生成嵌入 -> 批量插入 ES。\n",
    "\n",
    "**核心逻辑:**\n",
    "\n",
    "1.  **数据准备 (`process_item`):**\n",
    "    * 为每个文本块生成一个唯一的 `chunk_id` (例如使用 `xxhash`)。\n",
    "    * 整合块的元数据（内容 Tokens、文档名、文档 ID 等）。\n",
    "    * 调用 `generate_embedding` 获取向量。\n",
    "    * 构建符合 Elasticsearch 索引结构的字典。\n",
    "2.  **Elasticsearch 连接与设置:**\n",
    "    * 连接到 Elasticsearch 实例。\n",
    "    * （可选但推荐）定义索引的 `mapping`，明确字段类型（特别是向量字段）。\n",
    "    * 创建 Elasticsearch 索引。\n",
    "3.  **数据插入 (`insert`):**\n",
    "    * 使用 Elasticsearch 的 `bulk` API 批量插入准备好的数据，提高效率。\n",
    "    * 处理可能的插入错误和超时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2d07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "import logging\n",
    "\n",
    "ATTEMPT_TIME = 2\n",
    "\n",
    "logger = logging.getLogger('ragflow.es_conn')\n",
    "\n",
    "def process_item(item, file_name):\n",
    "    \"\"\"\n",
    "    处理单条数据\n",
    "    \"\"\"\n",
    "    try:      \n",
    "        content = item.get(\"content_with_weight\", \"\")\n",
    "        if not content:\n",
    "             print(\"Warning: Skipping item with empty 'content_with_weight'.\")\n",
    "             return None\n",
    "\n",
    "        # 生成 chunk_id using content and file_name\n",
    "        chunk_id = xxhash.xxh64((content + file_name).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        # 生成 doc_id based on file_name\n",
    "        doc_id = xxhash.xxh64(file_name.encode(\"utf-8\")).hexdigest()\n",
    "        # 构建数据字典\n",
    "        d = {\n",
    "            \"id\": chunk_id,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"docnm\": file_name,\n",
    "            \"title_tks\": item.get(\"title_tks\", []),\n",
    "            \"docnm_kwd\": item.get(\"docnm_kwd\", os.path.splitext(file_name)[0]), \n",
    "            \"content_ltks\": item[\"content_ltks\"],\n",
    "            \"content_with_weight\": item[\"content_with_weight\"],\n",
    "            \"content_sm_ltks\": item[\"content_sm_ltks\"],\n",
    "            \"important_kwd\": [],\n",
    "            \"important_tks\": [],\n",
    "            \"question_kwd\": [],\n",
    "            \"question_tks\": [],\n",
    "            \"create_time\": str(datetime.datetime.now()).replace(\"T\", \" \")[:19],\n",
    "            \"create_timestamp_flt\": datetime.datetime.now().timestamp()\n",
    "        }\n",
    "        # 生成并添加嵌入向量\n",
    "        embedding_vector = generate_embedding(content)\n",
    "       \n",
    "        if embedding_vector:\n",
    "            vector_field_name = f\"q_{len(embedding_vector)}_vec\"\n",
    "            d[vector_field_name] = embedding_vector\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Could not generate embedding for chunk {chunk_id}. Skipping vector field.\")\n",
    "            pass \n",
    "\n",
    "        return d\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"process_item error: {e}\")\n",
    "        return None\n",
    "        \n",
    "def parse(file_path):\n",
    "    # 使用自定义的 PDF 解析器\n",
    "    results = chunk(file_path, callback=dummy)\n",
    "    return results\n",
    "\n",
    "def insert(client, documents: list[dict], indexName: str) -> list[str]:\n",
    "        # Refers to https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html\n",
    "        operations = []\n",
    "        for d in documents:\n",
    "            assert \"_id\" not in d\n",
    "            assert \"id\" in d\n",
    "            d_copy = copy.deepcopy(d)\n",
    "            meta_id = d_copy.pop(\"id\", \"\")\n",
    "            operations.append(\n",
    "                {\"index\": {\"_index\": indexName, \"_id\": meta_id}})\n",
    "            operations.append(d_copy)\n",
    "\n",
    "        res = []\n",
    "        for _ in range(ATTEMPT_TIME):\n",
    "            try:\n",
    "                res = []\n",
    "                r = client.bulk(index=(indexName), operations=operations,\n",
    "                                 refresh=False, timeout=\"60s\")\n",
    "                if re.search(r\"False\", str(r[\"errors\"]), re.IGNORECASE):\n",
    "                    return res\n",
    "\n",
    "                for item in r[\"items\"]:\n",
    "                    for action in [\"create\", \"delete\", \"index\", \"update\"]:\n",
    "                        if action in item and \"error\" in item[action]:\n",
    "                            res.append(str(item[action][\"_id\"]) + \":\" + str(item[action][\"error\"]))\n",
    "                return res\n",
    "            except Exception as e:\n",
    "                res.append(str(e))\n",
    "                logger.warning(\"ESConnection.insert got exception: \" + str(e))\n",
    "                res = []\n",
    "                if re.search(r\"(Timeout|time out)\", str(e), re.IGNORECASE):\n",
    "                    res.append(str(e))\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "        return res\n",
    "\n",
    "\n",
    "def execute_insert_process(es_client, file_path, file_name, indexName):\n",
    "    \"\"\"\n",
    "    执行文档处理和插入 Elasticsearch 的函数.\n",
    "    如果索引存在，则先删除再插入。\n",
    "\n",
    "    :param es_client: elasticsearch client\n",
    "    :param file_path: 文件路径\n",
    "    :param file_name: 文件名 (用于 process_item)\n",
    "    :param indexName: 索引名称\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. 检查索引是否存在 (Check if the index exists)\n",
    "        if es_client.indices.exists(index=indexName):\n",
    "            logger.info(f\"Index '{indexName}' exists. Deleting it...\")\n",
    "            # 2. 如果存在，删除索引 (If it exists, delete the index)\n",
    "            # ignore=[400, 404] prevents errors if the index doesn't exist (race condition) or other delete issues\n",
    "            es_client.indices.delete(index=indexName, ignore=[400, 404])\n",
    "            logger.info(f\"Index '{indexName}' deleted.\")\n",
    "        else:\n",
    "            logger.info(f\"Index '{indexName}' does not exist. Proceeding with insertion.\")\n",
    "    except Exception as e:\n",
    "        # 3. 处理检查/删除索引时可能出现的错误 (Handle potential errors during check/delete)\n",
    "        logger.error(f\"Error checking or deleting index '{indexName}': {e}\")\n",
    "\n",
    "    documents = parse(file_path)\n",
    "    results = []\n",
    "    for item in documents:\n",
    "        processed_item = process_item(item, file_name)\n",
    "        results.append(processed_item)\n",
    "    # \n",
    "    insert(es_client, results, indexName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da9c8310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_component/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.3.174'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/var/folders/x9/3njcygzs5n9fqpx32r58686h0000gn/T/ipykernel_68416/93998078.py:116: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=indexName, ignore=[400, 404])\n",
      "/opt/anaconda3/envs/rag_component/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.3.174'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "WARNING:root:Miss outlines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理信息: OCR started\n",
      "处理进度: 60.0%\n",
      "处理信息: OCR finished (6.63s)\n",
      "处理进度: 63.0%\n",
      "处理信息: Layout analysis (3.51s)\n",
      "处理进度: 65.0%\n",
      "处理信息: Table analysis (0.16s)\n",
      "处理进度: 67.0%\n",
      "处理信息: Text merged (0.00s)\n",
      "naive_merge(example-pdf/report.pdf): 0.04160279082134366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_component/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.3.174'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import copy\n",
    "\n",
    "# Connect to 'http://localhost:9200'\n",
    "es_client = Elasticsearch(\n",
    "    \"https://192.168.3.174:9200/\", \n",
    "    verify_certs=False, \n",
    "    # api_key=(\"7Hb2FZYB9eekk6FT9B39\", \"pSBLeqtaRl2lfNsyng2NtA\"),\n",
    "    basic_auth=(\"elastic\", \"_9NsIQgJNFRFo7vKpk*L\")\n",
    "    )\n",
    "\n",
    "indexName = \"demo-index\"\n",
    "file_name = \"example-pdf/report.pdf\"\n",
    "file_path = os.path.join(os.path.dirname('__file__'), file_name)\n",
    "    \n",
    "execute_insert_process(es_client, file_path, file_name, indexName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd973c81",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "### 阶段五：检索 (Retrieval)\n",
    "\n",
    "当用户提出问题时，此阶段的目标是从 Elasticsearch 索引中找出与问题最相关的文本块。\n",
    "\n",
    "**核心逻辑:**\n",
    "\n",
    "1.  **查询嵌入:** 使用与索引时相同的 `generate_embedding` 函数，将用户的问题转换为查询向量。\n",
    "2.  **构建 ES 查询:**\n",
    "    * **向量相似度查询 (KNN):** 使用 `knn` 查询，基于查询向量查找 `k` 个最相似的文本块向量。`num_candidates` 参数可以提高查找的准确性（但会增加计算量）。\n",
    "    * **关键词查询 (可选):** 可以结合传统的关键词匹配查询（如 `match` 或 `bool` 查询）来过滤或增强结果。\n",
    "    * **过滤器 (可选):** 可以添加 `filter` 来限制搜索范围，例如只在特定的 `doc_id` 内搜索。\n",
    "3.  **执行查询:** 向 Elasticsearch 发送查询请求。\n",
    "4.  **提取结果:** 从 Elasticsearch 的响应中解析出相关的文本块内容及其元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46da33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    es_client,\n",
    "    index_name: str,\n",
    "    query_text: str,\n",
    "    keyword_field: str,\n",
    "    keyword_text: str,\n",
    "    vector_field: str,\n",
    "    k: int = 5,\n",
    "    num_candidates: int = 10,\n",
    "    size: int = 10,\n",
    "\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Performs a hybrid search (keyword + KNN vector search) on Elasticsearch.\n",
    "\n",
    "    Args:\n",
    "        es_client: An initialized Elasticsearch client instance.\n",
    "        index_name (str): The name of the Elasticsearch index to search.\n",
    "        query_text (str): The text query to generate the embedding from for KNN search.\n",
    "        keyword_field (str): The field to perform the keyword ('match') search on.\n",
    "        keyword_text (str): The text to search for in the keyword_field.\n",
    "        vector_field (str): The name of the dense vector field in the index.\n",
    "        k (int, optional): The number of nearest neighbors to return for KNN search. Defaults to 5.\n",
    "        num_candidates (int, optional): The number of candidates to consider for KNN search. Defaults to 10.\n",
    "        size (int, optional): The total number of hits to return. Defaults to 10.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search result hits, or None if an error occurs.\n",
    "              Each hit is a dictionary representing a document.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If embedding_function is not provided or fails.\n",
    "        Exception: For Elasticsearch search errors.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Generate the query vector using the provided function\n",
    "        query_vector = generate_embedding(query_text)\n",
    "\n",
    "        # 2. Define the KNN search part\n",
    "        knn_query = {\n",
    "            \"field\": vector_field,\n",
    "            \"query_vector\": query_vector,\n",
    "            \"k\": k,\n",
    "            \"num_candidates\": num_candidates,\n",
    "        }\n",
    "\n",
    "        # 3. Define the keyword search part\n",
    "        keyword_query = {\n",
    "            \"match\": {\n",
    "                keyword_field: keyword_text\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 4. Perform the search request\n",
    "        print(f\"Performing hybrid search on index '{index_name}'...\")\n",
    "        response = es_client.search(\n",
    "            index=index_name,\n",
    "            query=keyword_query, # Keyword query\n",
    "            knn=knn_query,       # KNN vector query\n",
    "            size=size,           # Total number of results to return\n",
    "            # request_timeout=30 # Optional: Set a timeout\n",
    "        )\n",
    "\n",
    "        # 5. Extract and return the results\n",
    "        results = response.get(\"hits\", {}).get(\"hits\", [])\n",
    "        print(f\"Found {len(results)} results.\")\n",
    "        return results\n",
    "\n",
    "    except ValueError as ve:\n",
    "         print(f\"Error during embedding generation: {ve}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Elasticsearch search: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf532ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hybrid search on index 'demo-index'...\n",
      "Found 3 results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_component/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.3.174'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# query = \"营业收入\"\n",
    "# keyword_text=\"2016年营业收入是多少？\"\n",
    "\n",
    "query = \"Compliant results\"\n",
    "keyword_text=\"What are the results of electrical insulation tests?\"\n",
    "\n",
    "results= hybrid_search(es_client=es_client,\n",
    "              index_name=indexName,\n",
    "              query_text=query,\n",
    "              keyword_field=\"content_with_weight\",\n",
    "              keyword_text=keyword_text,\n",
    "              vector_field=\"q_1024_vec\",\n",
    "              k=3,\n",
    "              num_candidates=5,\n",
    "              size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "751c2db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Details of the instrument calibration are maintained in laboratory records. Themeasurement of uncertaintyis availableuponrequest.IntroductionThis report describes the results of the test program conducted in accordance with ANSI Z89.1-2003 entitled,“American National Standard for Industrial Head Protection,\" performed on specimens submitted by the manufacturer.Testing of the abovementioned protective caps began only upon Interteks’receipt of the signed quote. Intertek Testing Services,located in Cortland NY,conducted the test evaluations.ProductDescriptionIntertek received 30 production protective caps with date code(s): 6/06 and 7/06.The test sampleswereidentified asspecimens1-30."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Requirements:Theprotective cap shall withstand2,200volts（rootmean square),AC,60Hertz,for1minutes.Leakage shall not exceed 3milliamperes.Results:ELECTRICALINSULATION (ClaSSE)Method:Theprotective capwas tested in accordance with Section9.7.Requirements:Theprotectivecap shall withstand 20,000volts（rootmean square),AC,60 Hertz,for3minutes.Leakage shall not exceed9milliamperes.At 30,000volts,the test sample shallnotburn through.Results:ConclusionThe protective cap identified as a Seong An Save,model Fashion I,met the minimum performancerequirements definedin ANSI Z89.1-2003 entitled,“American National Standard for Industrial HeadProtection”.The helmet type and class defined for this modelas a result of the evaluationsperformed in this report are determined tobeTypeI-Class G,E,& C."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<table><caption>PERFORMANCEREQUIREMENTS</caption>\n",
       "<tr><td  >SECTION</td><td  >COMPLIANT (yes/no)</td></tr>\n",
       "<tr><td  >6.1 -Each helmet shall be accompanied by manufacturers'instructions explaining the proper method of size adjustment,use,care and useful service life guidelines.</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-Each helmet shall bear permanent markings in at least1.5mm(0.06in.)high letters stating thefollowing information:</td><td></td></tr>\n",
       "<tr><td  >6.2-name or identification mark of themanufacturer;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-the date ofmanufacture;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2- the American National Standard Designation;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2- the applicable Type and Class Designations;</td><td  >Yes</td></tr>\n",
       "<tr><td  >6.2-the approximate headsize range (see table 1).</td><td  >Yes</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for result_ in results: \n",
    "    display(Markdown(result_[\"_source\"]['content_with_weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc44183",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "### 阶段六：生成 (Generation)\n",
    "\n",
    "最后一步，将用户原始问题和检索到的相关文本块（作为上下文）一起提供给大语言模型 (LLM)，让模型生成一个基于所提供信息的、连贯的答案。\n",
    "\n",
    "**核心逻辑:**\n",
    "\n",
    "1.  **构建 Prompt:** 创建一个清晰的指令，告诉 LLM 基于提供的上下文信息来回答指定的问题。\n",
    "2.  **配置 LLM 客户端:** 设置访问 LLM 服务的 API Key 和 Base URL。\n",
    "3.  **调用 LLM API:** 发送构建好的 Prompt 给 LLM。\n",
    "4.  **获取并展示答案:** 从 LLM 的响应中提取生成的文本答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a8181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_llm_answer(question, results_, api_key, base_url=\"https://api.deepseek.com\", model_name=\"deepseek-chat\"):\n",
    "\n",
    "    prompt = f\"\"\" Answer the question based on the information provided below.\\n{results_}\\nQuestion: {question}\"\"\"\n",
    "\n",
    "    llm_client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d07c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the electrical insulation tests for the protective cap are as follows:\n",
      "\n",
      "1. The protective cap withstood 2,200 volts (root mean square), AC, 60 Hertz for 1 minute, with leakage not exceeding 3 milliamperes. This meets the requirements for Class E electrical insulation.\n",
      "\n",
      "2. The protective cap also withstood 20,000 volts (root mean square), AC, 60 Hertz for 3 minutes, with leakage not exceeding 9 milliamperes. At 30,000 volts, the test sample did not burn through.\n",
      "\n",
      "These results confirm that the protective cap meets the minimum performance requirements defined in ANSI Z89.1-2003, entitled \"American National Standard for Industrial Head Protection,\" and is determined to be Type I-Class G, E, & C.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\", \"your_api_key_here\")\n",
    "\n",
    "\n",
    "question =\"what are the results of electrical insulation tests?\"\n",
    "\n",
    "response = get_llm_answer(question, results, api_key)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56db7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_component",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
